{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d46260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import operator\n",
    "import os\n",
    "import re\n",
    "from datetime import date, datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import TypedDict, List, Optional, Literal, Annotated\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7964d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Schemas\n",
    "# -----------------------------\n",
    "class Task(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "\n",
    "    goal: str = Field(\n",
    "        ...,\n",
    "        description=\"One sentence describing what the reader should be able to do/understand after this section.\",\n",
    "    )\n",
    "    bullets: List[str] = Field(\n",
    "        ...,\n",
    "        min_length=3,\n",
    "        max_length=6,\n",
    "        description=\"3–6 concrete, non-overlapping subpoints to cover in this section.\",\n",
    "    )\n",
    "    target_words: int = Field(..., description=\"Target word count for this section (120–550).\")\n",
    "\n",
    "    tags: List[str] = Field(default_factory=list)\n",
    "    requires_research: bool = False\n",
    "    requires_citations: bool = False\n",
    "    requires_code: bool = False\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    blog_title: str\n",
    "    audience: str\n",
    "    tone: str\n",
    "\n",
    "    # NEW: tells workers what genre this is (prevents drift)\n",
    "    blog_kind: Literal[\"explainer\", \"tutorial\", \"news_roundup\", \"comparison\", \"system_design\"] = \"explainer\"\n",
    "\n",
    "    constraints: List[str] = Field(default_factory=list)\n",
    "    tasks: List[Task]\n",
    "\n",
    "\n",
    "class EvidenceItem(BaseModel):\n",
    "    title: str\n",
    "    url: str\n",
    "    published_at: Optional[str] = None  # prefer ISO \"YYYY-MM-DD\"\n",
    "    snippet: Optional[str] = None\n",
    "    source: Optional[str] = None\n",
    "\n",
    "\n",
    "class RouterDecision(BaseModel):\n",
    "    needs_research: bool\n",
    "    mode: Literal[\"closed_book\", \"hybrid\", \"open_book\"]\n",
    "    reason: str\n",
    "    queries: List[str] = Field(default_factory=list)\n",
    "    max_results_per_query: int = Field(5, description=\"How many results to fetch per query (3–8).\")\n",
    "\n",
    "\n",
    "class EvidencePack(BaseModel):\n",
    "    evidence: List[EvidenceItem] = Field(default_factory=list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12e00b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "\n",
    "    # routing / research\n",
    "    mode: str\n",
    "    needs_research: bool\n",
    "    queries: List[str]\n",
    "    evidence: List[EvidenceItem]\n",
    "    plan: Optional[Plan]\n",
    "\n",
    "    # NEW: recency control\n",
    "    as_of: str           # ISO date, e.g. \"2026-01-29\"\n",
    "    recency_days: int    # 7 for weekly news, 30 for hybrid, etc.\n",
    "\n",
    "    # workers\n",
    "    sections: Annotated[List[tuple[int, str]], operator.add]  # (task_id, section_md)\n",
    "    final: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd530b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2) LLM\n",
    "# -----------------------------\n",
    "llm = ChatBedrockConverse(\n",
    "    model=\"us.anthropic.claude-sonnet-4-5-20250929-v1:0\",\n",
    "    region_name=os.getenv(\"AWS_DEFAULT_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY\"), # Use variables here\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62e1cc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3) Router (decide upfront)\n",
    "# -----------------------------\n",
    "ROUTER_SYSTEM = \"\"\"You are a routing module for a technical blog planner.\n",
    "\n",
    "Decide whether web research is needed BEFORE planning.\n",
    "\n",
    "Modes:\n",
    "- closed_book (needs_research=false):\n",
    "  Evergreen topics where correctness does not depend on recent facts (concepts, fundamentals).\n",
    "- hybrid (needs_research=true):\n",
    "  Mostly evergreen but needs up-to-date examples/tools/models to be useful.\n",
    "- open_book (needs_research=true):\n",
    "  Mostly volatile: weekly roundups, \"this week\", \"latest\", rankings, pricing, policy/regulation.\n",
    "\n",
    "If needs_research=true:\n",
    "- Output 3–10 high-signal queries.\n",
    "- Queries should be scoped and specific (avoid generic queries like just \"AI\" or \"LLM\").\n",
    "- For open_book weekly roundup, include queries that reflect the last 7 days constraint.\n",
    "\"\"\"\n",
    "\n",
    "def router_node(state: State) -> dict:\n",
    "    topic = state[\"topic\"]\n",
    "    decider = llm.with_structured_output(RouterDecision)\n",
    "    decision = decider.invoke(\n",
    "        [\n",
    "            SystemMessage(content=ROUTER_SYSTEM),\n",
    "            HumanMessage(content=f\"Topic: {topic}\\nAs-of date: {state['as_of']}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Set default recency window based on mode\n",
    "    if decision.mode == \"open_book\":\n",
    "        recency_days = 7\n",
    "    elif decision.mode == \"hybrid\":\n",
    "        recency_days = 45\n",
    "    else:\n",
    "        recency_days = 3650\n",
    "\n",
    "    return {\n",
    "        \"needs_research\": decision.needs_research,\n",
    "        \"mode\": decision.mode,\n",
    "        \"queries\": decision.queries,\n",
    "        \"recency_days\": recency_days,\n",
    "    }\n",
    "\n",
    "def route_next(state: State) -> str:\n",
    "    return \"research\" if state[\"needs_research\"] else \"orchestrator\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69c69df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4) Research (Tavily)\n",
    "# -----------------------------\n",
    "def _tavily_search(query: str, max_results: int = 5) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Uses TavilySearchResults if installed and TAVILY_API_KEY is set.\n",
    "    Returns list of dict with common fields. Note: published date is often missing.\n",
    "    \"\"\"\n",
    "    tool = TavilySearchResults(max_results=max_results)\n",
    "    results = tool.invoke({\"query\": query})\n",
    "\n",
    "    normalized: List[dict] = []\n",
    "    for r in results or []:\n",
    "        normalized.append(\n",
    "            {\n",
    "                \"title\": r.get(\"title\") or \"\",\n",
    "                \"url\": r.get(\"url\") or \"\",\n",
    "                \"snippet\": r.get(\"content\") or r.get(\"snippet\") or \"\",\n",
    "                \"published_at\": r.get(\"published_date\") or r.get(\"published_at\"),\n",
    "                \"source\": r.get(\"source\"),\n",
    "            }\n",
    "        )\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def _iso_to_date(s: Optional[str]) -> Optional[date]:\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        return date.fromisoformat(s[:10])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "RESEARCH_SYSTEM = \"\"\"You are a research synthesizer for technical writing.\n",
    "\n",
    "Given raw web search results, produce a deduplicated list of EvidenceItem objects.\n",
    "\n",
    "Rules:\n",
    "- Only include items with a non-empty url.\n",
    "- Prefer relevant + authoritative sources (company blogs, docs, reputable outlets).\n",
    "- Extract/normalize published_at as ISO (YYYY-MM-DD) if you can infer it from title/snippet.\n",
    "  If you can't infer a date reliably, set published_at=null (do NOT guess).\n",
    "- Keep snippets short.\n",
    "- Deduplicate by URL.\n",
    "\"\"\"\n",
    "\n",
    "def research_node(state: State) -> dict:\n",
    "    queries = (state.get(\"queries\", []) or [])[:10]\n",
    "    max_results = 6\n",
    "\n",
    "    raw_results: List[dict] = []\n",
    "    for q in queries:\n",
    "        raw_results.extend(_tavily_search(q, max_results=max_results))\n",
    "\n",
    "    if not raw_results:\n",
    "        return {\"evidence\": []}\n",
    "\n",
    "    extractor = llm.with_structured_output(EvidencePack)\n",
    "    pack = extractor.invoke(\n",
    "        [\n",
    "            SystemMessage(content=RESEARCH_SYSTEM),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"As-of date: {state['as_of']}\\n\"\n",
    "                    f\"Recency days: {state['recency_days']}\\n\\n\"\n",
    "                    f\"Raw results:\\n{raw_results}\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Deduplicate by URL\n",
    "    dedup = {}\n",
    "    for e in pack.evidence:\n",
    "        if e.url:\n",
    "            dedup[e.url] = e\n",
    "    evidence = list(dedup.values())\n",
    "\n",
    "    # HARD RECENCY FILTER for open_book weekly roundup:\n",
    "    # keep only items with a parseable ISO date and within the window.\n",
    "    mode = state.get(\"mode\", \"closed_book\")\n",
    "    if mode == \"open_book\":\n",
    "        as_of = date.fromisoformat(state[\"as_of\"])\n",
    "        cutoff = as_of - timedelta(days=int(state[\"recency_days\"]))\n",
    "        fresh: List[EvidenceItem] = []\n",
    "        for e in evidence:\n",
    "            d = _iso_to_date(e.published_at)\n",
    "            if d and d >= cutoff:\n",
    "                fresh.append(e)\n",
    "        evidence = fresh\n",
    "\n",
    "    return {\"evidence\": evidence}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c06809f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5) Orchestrator (Plan)\n",
    "# -----------------------------\n",
    "ORCH_SYSTEM = \"\"\"You are a senior technical writer and developer advocate.\n",
    "Your job is to produce a highly actionable outline for a technical blog post.\n",
    "\n",
    "Hard requirements:\n",
    "- Create 5–9 sections (tasks) suitable for the topic and audience.\n",
    "- Each task must include:\n",
    "  1) goal (1 sentence)\n",
    "  2) 3–6 bullets that are concrete, specific, and non-overlapping\n",
    "  3) target word count (120–550)\n",
    "\n",
    "Flexibility:\n",
    "- Do NOT use a fixed taxonomy unless it naturally fits.\n",
    "- You may tag tasks (tags field), but tags are flexible.\n",
    "\n",
    "Quality bar:\n",
    "- Assume the reader is a developer; use correct terminology.\n",
    "- Bullets must be actionable: build/compare/measure/verify/debug.\n",
    "- Ensure the overall plan includes at least 2 of these somewhere:\n",
    "  * minimal code sketch / MWE (set requires_code=True for that section)\n",
    "  * edge cases / failure modes\n",
    "  * performance/cost considerations\n",
    "  * security/privacy considerations (if relevant)\n",
    "  * debugging/observability tips\n",
    "\n",
    "Grounding rules:\n",
    "- Mode closed_book: keep it evergreen; do not depend on evidence.\n",
    "- Mode hybrid:\n",
    "  - Use evidence for up-to-date examples (models/tools/releases) in bullets.\n",
    "  - Mark sections using fresh info as requires_research=True and requires_citations=True.\n",
    "- Mode open_book (weekly news roundup):\n",
    "  - Set blog_kind = \"news_roundup\".\n",
    "  - Every section is about summarizing events + implications.\n",
    "  - DO NOT include tutorial/how-to sections (no scraping/RSS/how to fetch news) unless user explicitly asked for that.\n",
    "  - If evidence is empty or insufficient, create a plan that transparently says \"insufficient fresh sources\"\n",
    "    and includes only what can be supported.\n",
    "\n",
    "Output must strictly match the Plan schema.\n",
    "\"\"\"\n",
    "\n",
    "def orchestrator_node(state: State) -> dict:\n",
    "    planner = llm.with_structured_output(Plan)\n",
    "    evidence = state.get(\"evidence\", [])\n",
    "    mode = state.get(\"mode\", \"closed_book\")\n",
    "\n",
    "    # Force blog_kind for open_book\n",
    "    forced_kind = \"news_roundup\" if mode == \"open_book\" else None\n",
    "\n",
    "    plan = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=ORCH_SYSTEM),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Topic: {state['topic']}\\n\"\n",
    "                    f\"Mode: {mode}\\n\"\n",
    "                    f\"As-of: {state['as_of']} (recency_days={state['recency_days']})\\n\"\n",
    "                    f\"{'Force blog_kind=news_roundup' if forced_kind else ''}\\n\\n\"\n",
    "                    f\"Evidence (ONLY use for fresh claims; may be empty):\\n\"\n",
    "                    f\"{[e.model_dump() for e in evidence][:16]}\\n\\n\"\n",
    "                    f\"Instruction: If mode=open_book, your plan must NOT drift into a tutorial.\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Ensure open_book forces the kind even if model forgets\n",
    "    if forced_kind:\n",
    "        plan.blog_kind = \"news_roundup\"\n",
    "\n",
    "    return {\"plan\": plan}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Fanout\n",
    "# -----------------------------\n",
    "def fanout(state: State):\n",
    "    assert state[\"plan\"] is not None\n",
    "    return [\n",
    "        Send(\n",
    "            \"worker\",\n",
    "            {\n",
    "                \"task\": task.model_dump(),\n",
    "                \"topic\": state[\"topic\"],\n",
    "                \"mode\": state[\"mode\"],\n",
    "                \"as_of\": state[\"as_of\"],\n",
    "                \"recency_days\": state[\"recency_days\"],\n",
    "                \"plan\": state[\"plan\"].model_dump(),\n",
    "                \"evidence\": [e.model_dump() for e in state.get(\"evidence\", [])],\n",
    "            },\n",
    "        )\n",
    "        for task in state[\"plan\"].tasks\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6f50a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 7) Worker (write one section)\n",
    "# -----------------------------\n",
    "WORKER_SYSTEM = \"\"\"You are a senior technical writer and developer advocate.\n",
    "Write ONE section of a technical blog post in Markdown.\n",
    "\n",
    "Hard constraints:\n",
    "- Follow the provided Goal and cover ALL Bullets in order (do not skip or merge bullets).\n",
    "- Stay close to Target words (±15%).\n",
    "- Output ONLY the section content in Markdown (no blog title H1, no extra commentary).\n",
    "- Start with a '## <Section Title>' heading.\n",
    "\n",
    "Scope guard (prevents mid-blog topic drift):\n",
    "- If blog_kind == \"news_roundup\": do NOT turn this into a tutorial/how-to guide.\n",
    "  Do NOT teach web scraping, RSS, automation, or \"how to fetch news\" unless bullets explicitly ask for it.\n",
    "  Focus on summarizing events and implications.\n",
    "\n",
    "Grounding policy:\n",
    "- If mode == open_book (weekly news):\n",
    "  - Do NOT introduce any specific event/company/model/funding/policy claim unless it is supported by provided Evidence URLs.\n",
    "  - For each event claim, attach a source as a Markdown link: ([Source](URL)).\n",
    "  - Only use URLs provided in Evidence. If not supported, write: \"Not found in provided sources.\"\n",
    "- If requires_citations == true (hybrid sections):\n",
    "  - For outside-world claims, cite Evidence URLs the same way.\n",
    "- Evergreen reasoning (concepts, intuition) is OK without citations unless requires_citations is true.\n",
    "\n",
    "Code:\n",
    "- If requires_code == true, include at least one minimal, correct code snippet relevant to the bullets.\n",
    "\n",
    "Style:\n",
    "- Short paragraphs, bullets where helpful, code fences for code.\n",
    "- Avoid fluff/marketing. Be precise and implementation-oriented.\n",
    "\"\"\"\n",
    "\n",
    "def worker_node(payload: dict) -> dict:\n",
    "    \n",
    "    task = Task(**payload[\"task\"])\n",
    "    plan = Plan(**payload[\"plan\"])\n",
    "    evidence = [EvidenceItem(**e) for e in payload.get(\"evidence\", [])]\n",
    "    topic = payload[\"topic\"]\n",
    "    mode = payload.get(\"mode\", \"closed_book\")\n",
    "    as_of = payload.get(\"as_of\")\n",
    "    recency_days = payload.get(\"recency_days\")\n",
    "\n",
    "    bullets_text = \"\\n- \" + \"\\n- \".join(task.bullets)\n",
    "\n",
    "    # Provide a compact evidence list for citation use\n",
    "    evidence_text = \"\"\n",
    "    if evidence:\n",
    "        evidence_text = \"\\n\".join(\n",
    "            f\"- {e.title} | {e.url} | {e.published_at or 'date:unknown'}\".strip()\n",
    "            for e in evidence[:20]\n",
    "        )\n",
    "\n",
    "    section_md = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=WORKER_SYSTEM),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Blog title: {plan.blog_title}\\n\"\n",
    "                    f\"Audience: {plan.audience}\\n\"\n",
    "                    f\"Tone: {plan.tone}\\n\"\n",
    "                    f\"Blog kind: {plan.blog_kind}\\n\"\n",
    "                    f\"Constraints: {plan.constraints}\\n\"\n",
    "                    f\"Topic: {topic}\\n\"\n",
    "                    f\"Mode: {mode}\\n\"\n",
    "                    f\"As-of: {as_of} (recency_days={recency_days})\\n\\n\"\n",
    "                    f\"Section title: {task.title}\\n\"\n",
    "                    f\"Goal: {task.goal}\\n\"\n",
    "                    f\"Target words: {task.target_words}\\n\"\n",
    "                    f\"Tags: {task.tags}\\n\"\n",
    "                    f\"requires_research: {task.requires_research}\\n\"\n",
    "                    f\"requires_citations: {task.requires_citations}\\n\"\n",
    "                    f\"requires_code: {task.requires_code}\\n\"\n",
    "                    f\"Bullets:{bullets_text}\\n\\n\"\n",
    "                    f\"Evidence (ONLY use these URLs when citing):\\n{evidence_text}\\n\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    ).content.strip()\n",
    "\n",
    "    # deterministic ordering\n",
    "    return {\"sections\": [(task.id, section_md)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1f45bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 8) Reducer (merge + save)\n",
    "# -----------------------------\n",
    "def reducer_node(state: State) -> dict:\n",
    "    plan = state[\"plan\"]\n",
    "    if plan is None:\n",
    "        raise ValueError(\"Reducer called without a plan.\")\n",
    "\n",
    "    ordered_sections = [md for _, md in sorted(state[\"sections\"], key=lambda x: x[0])]\n",
    "    body = \"\\n\\n\".join(ordered_sections).strip()\n",
    "    final_md = f\"# {plan.blog_title}\\n\\n{body}\\n\"\n",
    "\n",
    "    blog_dir = Path(\"blogs\")\n",
    "    blog_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    clean_title = \"\".join(c if c.isalnum() or c in (\" \", \"_\", \"-\") else \"\" for c in plan.blog_title)\n",
    "    filename = clean_title.strip().lower().replace(\" \", \"_\") + \".md\"\n",
    "    \n",
    "    file_path = blog_dir / filename\n",
    "    file_path.write_text(final_md, encoding=\"utf-8\")\n",
    "    \n",
    "    print(f\"Successfully saved blog to: {file_path}\")\n",
    "\n",
    "    return {\"final\": final_md}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "adfb355c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000001EC7C2D1F30>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 9) Build graph\n",
    "# -----------------------------\n",
    "g = StateGraph(State)\n",
    "g.add_node(\"router\", router_node)\n",
    "g.add_node(\"research\", research_node)\n",
    "g.add_node(\"orchestrator\", orchestrator_node)\n",
    "g.add_node(\"worker\", worker_node)\n",
    "g.add_node(\"reducer\", reducer_node)\n",
    "\n",
    "g.add_edge(START, \"router\")\n",
    "g.add_conditional_edges(\"router\", route_next, {\"research\": \"research\", \"orchestrator\": \"orchestrator\"})\n",
    "g.add_edge(\"research\", \"orchestrator\")\n",
    "\n",
    "g.add_conditional_edges(\"orchestrator\", fanout, [\"worker\"])\n",
    "g.add_edge(\"worker\", \"reducer\")\n",
    "g.add_edge(\"reducer\", END)\n",
    "\n",
    "app = g.compile()\n",
    "\n",
    "app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "430f5bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 10) Runner\n",
    "# -----------------------------\n",
    "def run(topic: str, as_of: Optional[str] = None):\n",
    "    if as_of is None:\n",
    "        as_of = date.today().isoformat()\n",
    "\n",
    "    out = app.invoke(\n",
    "        {\n",
    "            \"topic\": topic,\n",
    "            \"mode\": \"\",\n",
    "            \"needs_research\": False,\n",
    "            \"queries\": [],\n",
    "            \"evidence\": [],\n",
    "            \"plan\": None,\n",
    "            \"as_of\": as_of,\n",
    "            \"recency_days\": 7,   # router may overwrite\n",
    "            \"sections\": [],\n",
    "            \"final\": \"\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    plan: Plan = out[\"plan\"]\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"TOPIC:\", topic)\n",
    "    print(\"AS_OF:\", out.get(\"as_of\"), \"RECENCY_DAYS:\", out.get(\"recency_days\"))\n",
    "    print(\"MODE:\", out.get(\"mode\"))\n",
    "    print(\"BLOG_KIND:\", plan.blog_kind)\n",
    "    print(\"NEEDS_RESEARCH:\", out.get(\"needs_research\"))\n",
    "    print(\"QUERIES:\", (out.get(\"queries\") or [])[:6])\n",
    "    print(\"EVIDENCE_COUNT:\", len(out.get(\"evidence\", [])))\n",
    "    if out.get(\"evidence\"):\n",
    "        print(\"EVIDENCE_SAMPLE:\", [e.model_dump() for e in out[\"evidence\"][:2]])\n",
    "    print(\"TASKS:\", len(plan.tasks))\n",
    "    print(\"SAVED_MD_CHARS:\", len(out.get(\"final\", \"\")))\n",
    "    print(\"=\" * 100 + \"\\n\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5be9f123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved blog to: blogs\\understanding_self-attention_the_mechanism_behind_modern_transformers.md\n",
      "\n",
      "====================================================================================================\n",
      "TOPIC: Write a blog on Self Attention\n",
      "AS_OF: 2026-02-01 RECENCY_DAYS: 3650\n",
      "MODE: closed_book\n",
      "BLOG_KIND: explainer\n",
      "NEEDS_RESEARCH: False\n",
      "QUERIES: []\n",
      "EVIDENCE_COUNT: 0\n",
      "TASKS: 9\n",
      "SAVED_MD_CHARS: 24034\n",
      "====================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'topic': 'Write a blog on Self Attention',\n",
       " 'mode': 'closed_book',\n",
       " 'needs_research': False,\n",
       " 'queries': [],\n",
       " 'evidence': [],\n",
       " 'plan': Plan(blog_title='Understanding Self-Attention: The Mechanism Behind Modern Transformers', audience='Software engineers and ML practitioners who want to understand the self-attention mechanism from first principles, including those implementing transformers or working with LLMs', tone='Technical but accessible, focusing on intuition first then diving into mechanics. Use clear analogies and concrete examples. Balance mathematical notation with practical implementation details.', blog_kind='explainer', constraints=[], tasks=[Task(id=1, title='The Problem Self-Attention Solves', goal='Readers should understand why recurrent and convolutional approaches were limiting and what specific computational problem self-attention addresses.', bullets=['Sequential bottleneck in RNNs: why processing tokens one-at-a-time creates training parallelization issues and long-range dependency degradation', 'Limited receptive fields in CNNs: how fixed-window convolutions struggle with variable-length dependencies', 'The core insight: learning which parts of the input to attend to dynamically, regardless of position', \"Concrete example: resolving pronoun references in 'The animal didn't cross the street because it was too tired' requires flexible attention spans\"], target_words=300, tags=['motivation', 'background'], requires_research=False, requires_citations=False, requires_code=False), Task(id=2, title='Self-Attention Mechanics: Queries, Keys, and Values', goal='Readers should grasp the QKV framework and understand how attention scores are computed and applied.', bullets=['The information retrieval metaphor: queries search over keys to retrieve values', 'Three linear projections from input embeddings: Wq, Wk, Wv matrices transform each token', 'Similarity scoring via dot product: computing query·key^T to measure relevance between all token pairs', 'Softmax normalization: converting raw scores into a probability distribution over positions', 'Weighted aggregation: combining values according to attention weights to produce contextualized output', 'Walk through a minimal 3-token example with concrete numbers showing dimension changes'], target_words=450, tags=['core-concept', 'mechanics'], requires_research=False, requires_citations=False, requires_code=False), Task(id=3, title='Implementing Scaled Dot-Product Attention', goal=\"Readers should be able to implement the core attention function and understand the scaling factor's purpose.\", bullets=['NumPy/PyTorch code for attention(Q, K, V) showing matrix dimensions at each step', 'The sqrt(d_k) scaling factor: why dividing by dimension prevents softmax saturation in high dimensions', 'Handling batched inputs: reshaping for (batch, seq_len, d_model) tensors', 'Optional masking: implementing causal masks for autoregressive generation and padding masks', 'Verifying output shapes and attention weight matrix properties (rows sum to 1)'], target_words=400, tags=['implementation', 'code'], requires_research=False, requires_citations=False, requires_code=True), Task(id=4, title='Multi-Head Attention: Parallel Representation Subspaces', goal=\"Readers should understand why multiple attention heads improve expressiveness and how they're implemented efficiently.\", bullets=['Motivation: single attention may focus on syntax while another captures semantics; different heads learn different relationship types', 'Splitting d_model into h heads: each head operates on d_k = d_model/h dimensions independently', 'Parallel computation: reshaping tensors to process all heads simultaneously rather than loops', 'Concatenation and output projection: merging head outputs and applying final linear transformation', 'Visualization insight: empirically observed specialization where heads attend to different linguistic phenomena'], target_words=350, tags=['architecture', 'multi-head'], requires_research=False, requires_citations=False, requires_code=False), Task(id=5, title='Computational Complexity and Memory Footprint', goal=\"Readers should understand self-attention's O(n²) bottleneck and recognize when it becomes prohibitive.\", bullets=['Quadratic complexity analysis: O(n²·d) for computing attention scores on sequence length n', 'Memory requirements: storing the n×n attention matrix for backpropagation', 'Comparison with RNN O(n·d²) and CNN O(n·k·d²) where k is kernel width', 'Practical break-even points: when self-attention becomes expensive (typically >2048 tokens with standard hardware)', 'Brief mention of efficiency techniques: sparse attention patterns, linear attention approximations, and memory-efficient attention implementations'], target_words=320, tags=['performance', 'complexity'], requires_research=False, requires_citations=False, requires_code=False), Task(id=6, title='Positional Information and Permutation Invariance', goal='Readers should recognize that self-attention is position-agnostic by default and understand why positional encodings are necessary.', bullets=['Self-attention as a set operation: outputs are invariant to input reordering without position signals', \"Why position matters: 'dog bites man' vs 'man bites dog' have identical self-attention outputs without positional encoding\", 'Absolute positional encodings: sinusoidal functions and learned embeddings added to input', 'Relative position representations: building position awareness directly into attention computation', 'Trade-offs: absolute encodings are simpler but relative can generalize better to unseen sequence lengths'], target_words=300, tags=['architecture', 'positional-encoding'], requires_research=False, requires_citations=False, requires_code=False), Task(id=7, title='Common Implementation Pitfalls and Debugging', goal='Readers should be able to diagnose and fix typical bugs when implementing or using self-attention.', bullets=['Attention weight degeneracy: all-uniform or single-peaked distributions indicating optimization failure or improper scaling', 'Dimension mismatches: tracking batch/sequence/feature dimensions through reshapes, especially in multi-head splitting', 'Mask errors: inverted masks (attending to padding instead of content) or incorrect mask broadcasting', \"Gradient vanishing: checking attention entropy and ensuring softmax isn't saturating due to large logits\", 'Numerical stability: addressing exp overflow in softmax with the log-sum-exp trick', 'Verification tests: ensuring attention weights sum to 1.0 per row and masked positions are truly zero'], target_words=380, tags=['debugging', 'implementation', 'edge-cases'], requires_research=False, requires_citations=False, requires_code=False), Task(id=8, title='Self-Attention Variants and Extensions', goal='Readers should be aware of important variations built on the core mechanism and when to consider them.', bullets=['Cross-attention: using queries from one sequence and keys/values from another (encoder-decoder, retrieval-augmented generation)', 'Sparse attention: restricting attention to local windows or structured patterns to reduce O(n²) cost', 'Flash Attention and memory-efficient implementations: computing attention without materializing the full matrix', 'Grouped-query attention: sharing key/value projections across multiple query heads to reduce KV cache size in inference'], target_words=280, tags=['variants', 'extensions'], requires_research=False, requires_citations=False, requires_code=False), Task(id=9, title='When to Use Self-Attention vs Alternatives', goal='Readers should be able to make informed architectural decisions based on task requirements and constraints.', bullets=['Ideal use cases: variable-length dependencies, parallel training on sequences, tasks requiring global context', 'When RNNs/LSTMs still make sense: streaming applications with strict left-to-right processing, extremely long sequences with limited compute', 'Hybrid architectures: combining convolutions for local feature extraction with self-attention for global relationships (ConvNets + transformers)', 'Practical constraints: balancing model quality against inference latency and memory budgets in production', 'Domain considerations: vision (ViT vs CNN), audio (conformers), and structured data where relational inductive biases matter'], target_words=340, tags=['decision-making', 'architecture'], requires_research=False, requires_citations=False, requires_code=False)]),\n",
       " 'as_of': '2026-02-01',\n",
       " 'recency_days': 3650,\n",
       " 'sections': [(1,\n",
       "   '## The Problem Self-Attention Solves\\n\\nBefore transformers revolutionized natural language processing, two architectures dominated sequence modeling: recurrent neural networks (RNNs) and convolutional neural networks (CNNs). Both faced fundamental limitations that self-attention was designed to overcome.\\n\\n**The Sequential Bottleneck**\\n\\nRNNs process sequences token-by-token, maintaining a hidden state that flows from one timestep to the next. This sequential dependency creates a critical bottleneck: you cannot compute the representation for token 100 until you\\'ve processed tokens 1 through 99. During training, this means no parallelization across the sequence dimension—you\\'re stuck processing one step at a time, even with modern GPU hardware.\\n\\nWorse, as sequences grow longer, information must pass through many intermediate states. The hidden state acts like a telephone game where the message degrades over distance. By the time an RNN processes the 100th token, subtle information from token 5 has been compressed and recompressed through 95 sequential transformations, making long-range dependencies difficult to learn.\\n\\n**Fixed Windows and Receptive Fields**\\n\\nCNNs avoid sequential processing by applying filters in parallel, but they introduce a different constraint: fixed receptive fields. A convolutional layer with kernel size 3 only sees three adjacent tokens. To capture dependencies across 20 tokens, you need to stack multiple layers, and the receptive field grows slowly. The architecture imposes a rigid geometric structure on which tokens can interact.\\n\\n**The Core Insight**\\n\\nSelf-attention solves both problems with a deceptively simple idea: let every token directly compare itself to every other token in the sequence, and learn which positions matter. Instead of processing left-to-right or through fixed windows, the model computes attention scores dynamically based on content similarity.\\n\\nConsider the sentence: \"The animal didn\\'t cross the street because it was too tired.\" To understand what \"it\" refers to, you need to connect tokens 8 (\"it\") and 2 (\"animal\"), skipping over intervening words. Self-attention allows the model to learn this connection directly, regardless of distance, in a single operation that parallelizes across all tokens simultaneously.'),\n",
       "  (2,\n",
       "   '## Self-Attention Mechanics: Queries, Keys, and Values\\n\\nThe self-attention mechanism borrows its core metaphor from information retrieval systems. Imagine searching a library: you have a **query** (what you\\'re looking for), **keys** (index cards describing each book), and **values** (the actual book contents). Self-attention works the same way—each token generates a query to search over all other tokens\\' keys, then retrieves a weighted combination of their values.\\n\\n### The Three Linear Projections\\n\\nSelf-attention starts by transforming each token\\'s embedding through three separate learned weight matrices: **Wq** (query), **Wk** (key), and **Wv** (value). If your input embeddings have dimension d_model, these matrices typically project to a smaller dimension d_k (often d_model/num_heads).\\n\\nFor an input token embedding **x**, we compute:\\n- Query: **q** = **x** · Wq\\n- Key: **k** = **x** · Wk  \\n- Value: **v** = **x** · Wv\\n\\nEvery token in the sequence undergoes this transformation independently, producing matrices Q, K, and V where each row corresponds to one token\\'s projection.\\n\\n### Computing Attention Scores\\n\\nThe heart of self-attention is measuring how much each token should attend to every other token. We compute similarity by taking the dot product between each query and all keys: **q** · **k**^T. High dot products indicate strong relevance; the query \"found\" something useful in that key.\\n\\nFor a sequence of n tokens, this produces an n×n matrix of raw attention scores. Each row represents one query\\'s scores across all keys—essentially asking \"how relevant is every position to this position?\"\\n\\nThese raw scores get scaled by 1/√d_k to prevent extremely large values that would make gradients vanish during training. Then we apply **softmax** row-wise, converting each row of scores into a probability distribution that sums to 1. This normalization ensures attention weights are interpretable and numerically stable.\\n\\n### Weighted Value Aggregation\\n\\nThe final step multiplies the attention weights by the value matrix V. Each output position receives a weighted sum of all value vectors, where the weights come from the normalized attention scores. Positions with high attention weights contribute more to the output.\\n\\n### Concrete Example\\n\\nConsider three tokens with 4-dimensional embeddings. After projections to d_k=2:\\n\\n```\\nQ = [[1.0, 0.5],     K = [[1.0, 0.2],     V = [[2.0, 1.0],\\n     [0.3, 0.8],          [0.5, 0.9],          [1.5, 0.5],\\n     [0.6, 0.4]]          [0.4, 0.3]]          [1.0, 2.0]]\\n```\\n\\nCompute scores: Q·K^T / √2 gives a 3×3 matrix. For token 0: [1.0×1.0 + 0.5×0.2, 1.0×0.5 + 0.5×0.9, ...] = [0.78, 0.67, 0.53]. After softmax: [0.37, 0.33, 0.30]. The output for token 0 becomes: 0.37×[2.0,1.0] + 0.33×[1.5,0.5] + 0.30×[1.0,2.0] ≈ [1.64, 1.14], a context-aware representation blending information from all positions.'),\n",
       "  (3,\n",
       "   '## Implementing Scaled Dot-Product Attention\\n\\nLet\\'s build the core attention mechanism from scratch. We\\'ll start with a clean NumPy implementation, then show the PyTorch equivalent that handles batching efficiently.\\n\\nHere\\'s the fundamental scaled dot-product attention function:\\n\\n```python\\nimport numpy as np\\n\\ndef attention(Q, K, V):\\n    \"\"\"\\n    Q: Query matrix of shape (seq_len, d_k)\\n    K: Key matrix of shape (seq_len, d_k)\\n    V: Value matrix of shape (seq_len, d_v)\\n    Returns: Output of shape (seq_len, d_v), attention weights\\n    \"\"\"\\n    d_k = Q.shape[-1]\\n    \\n    # Step 1: Compute attention scores - shape (seq_len, seq_len)\\n    scores = Q @ K.T  # Matrix multiplication\\n    \\n    # Step 2: Scale by sqrt(d_k) - shape unchanged\\n    scaled_scores = scores / np.sqrt(d_k)\\n    \\n    # Step 3: Apply softmax row-wise - shape (seq_len, seq_len)\\n    attention_weights = np.exp(scaled_scores) / np.exp(scaled_scores).sum(axis=-1, keepdims=True)\\n    \\n    # Step 4: Weight the values - shape (seq_len, d_v)\\n    output = attention_weights @ V\\n    \\n    return output, attention_weights\\n```\\n\\n**Why the sqrt(d_k) scaling factor?** Without scaling, dot products grow large as dimensionality increases. Consider two random unit vectors in high dimensions—their dot product variance scales with `d_k`. Large scores push softmax into regions with vanishingly small gradients (imagine softmax([100, 1, 1])—it\\'s essentially a hard selection with near-zero gradient for non-max positions). Dividing by `sqrt(d_k)` keeps the variance of scores roughly constant regardless of dimension, maintaining healthy gradients during training.\\n\\nFor production use with batched inputs, here\\'s the PyTorch implementation:\\n\\n```python\\nimport torch\\nimport torch.nn.functional as F\\n\\ndef scaled_dot_product_attention(Q, K, V, mask=None):\\n    \"\"\"\\n    Q, K, V: shape (batch_size, seq_len, d_k) or (batch_size, num_heads, seq_len, d_k)\\n    mask: optional boolean mask, shape (seq_len, seq_len) or (batch_size, seq_len, seq_len)\\n    \"\"\"\\n    d_k = Q.size(-1)\\n    \\n    # scores: (batch, seq_len, seq_len) or (batch, num_heads, seq_len, seq_len)\\n    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\\n    \\n    # Apply mask before softmax (set masked positions to -inf)\\n    if mask is not None:\\n        scores = scores.masked_fill(mask == 0, float(\\'-inf\\'))\\n    \\n    attention_weights = F.softmax(scores, dim=-1)\\n    output = torch.matmul(attention_weights, V)\\n    \\n    return output, attention_weights\\n```\\n\\n**Masking for different use cases:** Causal masks prevent attention to future positions (essential for autoregressive generation like GPT):\\n\\n```python\\ndef create_causal_mask(seq_len):\\n    # Lower triangular matrix: position i can only attend to j <= i\\n    return torch.tril(torch.ones(seq_len, seq_len)).bool()\\n\\n# Padding masks ignore <PAD> tokens\\ndef create_padding_mask(seq_lengths, max_len):\\n    batch_size = len(seq_lengths)\\n    mask = torch.arange(max_len).expand(batch_size, max_len) < seq_lengths.unsqueeze(1)\\n    return mask.unsqueeze(1)  # (batch, 1, seq_len) for broadcasting\\n```\\n\\n**Verification checks:** The attention weight matrix should always satisfy two properties: (1) each row sums to 1.0 (it\\'s a probability distribution), and (2) all values are non-negative. You can verify with:\\n\\n```python\\n# After computing attention_weights\\nassert torch.allclose(attention_weights.sum(dim=-1), torch.ones(attention_weights.shape[:-1]))\\nassert (attention_weights >= 0).all()\\n```\\n\\nThe output shape should match the value dimension: if V has shape `(batch, seq_len, d_v)`, output will too. Each output position is a weighted sum of all value vectors, where weights come from comparing that position\\'s query against all keys.'),\n",
       "  (4,\n",
       "   '## Multi-Head Attention: Parallel Representation Subspaces\\n\\nSingle attention heads face a fundamental limitation: they must make trade-offs about what patterns to capture. A single set of query, key, and value projections might learn to identify subject-verb relationships in a sentence, but simultaneously capturing semantic similarity, syntactic structure, and positional patterns proves difficult. Multi-head attention solves this by running several attention mechanisms in parallel, each learning to specialize in different types of relationships.\\n\\n**Representation Splitting Across Heads**\\n\\nInstead of computing attention once with full dimensionality, multi-head attention divides the model dimension `d_model` into `h` independent heads. Each head operates on a smaller dimension `d_k = d_model / h`. For example, with `d_model = 512` and `h = 8` heads, each head works with `d_k = 64` dimensions.\\n\\nThe key insight is that each head gets its own learned query, key, and value projection matrices. Head 1 might learn projections that emphasize word order and position, while Head 2\\'s projections capture semantic relationships between entities. Head 3 might specialize in long-range dependencies. This division creates independent \"representation subspaces\" where different aspects of the input can be captured simultaneously.\\n\\n**Efficient Parallel Implementation**\\n\\nRather than iterating through heads sequentially, implementations reshape tensors to compute all heads at once. After projecting queries, keys, and values, the tensors are reshaped from `[batch, seq_len, d_model]` to `[batch, num_heads, seq_len, d_k]`. This reorganization allows the matrix multiplication operations in scaled dot-product attention to process all heads in parallel using batched operations, maintaining computational efficiency despite running multiple attention mechanisms.\\n\\n**Combining Head Outputs**\\n\\nAfter each head computes its attention-weighted values, the outputs are concatenated back together. If Head 1 produces a `[batch, seq_len, d_k]` tensor and we have `h` heads, concatenation yields `[batch, seq_len, h * d_k] = [batch, seq_len, d_model]`. A final learned linear projection `W_O` then mixes information across all heads, allowing them to jointly determine the output representation.\\n\\n**Empirical Specialization Patterns**\\n\\nAnalysis of trained transformers reveals fascinating head specialization. Some heads consistently attend to the previous token (capturing sequential dependencies), others attend to sentence-ending punctuation (syntax structure), and some form broad attention patterns capturing semantic fields. This emergent specialization, learned purely from data without explicit supervision, demonstrates why multiple heads substantially improve model expressiveness compared to single-head architectures.'),\n",
       "  (5,\n",
       "   \"## Computational Complexity and Memory Footprint\\n\\nSelf-attention's elegant design comes with a computational price tag that every practitioner must understand. The mechanism computes pairwise relationships between all tokens in a sequence, leading to **quadratic complexity** in both time and space.\\n\\n### Time Complexity Analysis\\n\\nFor a sequence of length `n` with embedding dimension `d`, computing the full attention matrix requires O(n²·d) operations. Here's the breakdown:\\n\\n- **Query-Key dot products**: n² dot products, each requiring d multiplications → O(n²·d)\\n- **Softmax normalization**: Applied across n² scores → O(n²)\\n- **Value aggregation**: n² weighted combinations of d-dimensional vectors → O(n²·d)\\n\\nThe dominant term is O(n²·d), meaning doubling your sequence length quadruples the compute time.\\n\\n### Memory Requirements\\n\\nThe n×n attention matrix must be materialized and stored during the forward pass for gradient computation during backpropagation. For a 2048-token sequence, that's 4 million floating-point values per attention head. With 8 heads and 32-bit floats, you're storing ~128MB just for attention weights in a single layer. Deep models with 24+ layers compound this quickly.\\n\\n### Comparison with Alternatives\\n\\nThis contrasts sharply with sequential architectures:\\n\\n- **RNNs**: O(n·d²) complexity—linear in sequence length but quadratic in hidden dimension\\n- **CNNs**: O(n·k·d²) where k is the kernel width (typically 3-7)—linear in sequence length with localized context\\n\\nFor short sequences (n < 512) with large hidden dimensions (d ≈ 1024), RNNs can actually be more expensive per token. But as sequences grow, self-attention's quadratic term dominates.\\n\\n### Practical Break-Even Points\\n\\nOn standard GPUs (V100/A100), self-attention becomes prohibitively expensive beyond 2048 tokens for training, and inference slows noticeably past 4096 tokens. These limits have driven innovation in **efficiency techniques**: sparse attention patterns that compute only a subset of token pairs, linear attention approximations that reduce complexity to O(n·d²), and memory-efficient attention implementations like Flash Attention that reduce memory overhead through kernel fusion and recomputation strategies.\"),\n",
       "  (6,\n",
       "   '## Positional Information and Permutation Invariance\\n\\nA fundamental property of self-attention that often surprises newcomers: **the mechanism is completely position-agnostic**. If you shuffle the input tokens randomly, self-attention produces the exact same outputs in the shuffled order. This permutation invariance stems from how attention treats inputs as an unordered set—each token attends to all others based purely on content similarity, with no inherent notion of which token came \"before\" or \"after\" another.\\n\\nThis creates an immediate problem for language understanding. The sentences \"dog bites man\" and \"man bites dog\" convey opposite meanings, yet without positional information, self-attention computes identical representations for both. Each word looks at the same neighboring words and generates the same attention weights, just in different orders. The mechanism has no way to distinguish sequential relationships that fundamentally change meaning.\\n\\nTo restore position awareness, transformers inject **positional encodings** into the input embeddings before attention begins. The most common approaches fall into two categories:\\n\\n**Absolute positional encodings** assign each position an embedding vector that gets added to the token embedding. The original Transformer paper introduced sinusoidal functions—deterministic patterns at different frequencies that let the model distinguish positions mathematically. Alternatively, many modern models use learned position embeddings, treating them as trainable parameters optimized during training. These approaches are simple to implement: just add the position vector to each token embedding before the first attention layer.\\n\\n**Relative position representations** take a different approach by modifying the attention mechanism itself to account for the distance between tokens. Instead of asking \"what\\'s at position 5?\", the model asks \"what\\'s 3 positions away from me?\" This can be implemented by adjusting attention scores based on relative offsets or by incorporating relative position embeddings into the key/query computations.\\n\\nThe trade-off centers on generalization. Absolute encodings are straightforward but struggle when inference sequences exceed training lengths—position 10,000 looks alien if you only trained on sequences up to 2,048 tokens. Relative encodings can naturally extrapolate since they reason about distances rather than absolute indices, though they add computational complexity to the attention calculation itself. Modern architectures like RoPE (Rotary Position Embedding) attempt to get the best of both worlds by encoding relative information through rotation matrices applied to query and key vectors.'),\n",
       "  (7,\n",
       "   '## Common Implementation Pitfalls and Debugging\\n\\nWhen implementing self-attention from scratch, several failure modes appear repeatedly. Recognizing these patterns early saves hours of head-scratching debugging.\\n\\n**Attention weight degeneracy** manifests as pathological distributions—either all weights collapsing to uniform values (1/N across all positions) or spiking to a single position. The uniform case often signals dead neurons or vanishing gradients upstream. Single-peaked attention may indicate runaway scaling: if your attention logits grow too large before softmax, numerical precision causes one weight to dominate. Check your scaling factor—√d_k is standard, but verify it matches your actual key dimension. Inspect raw logit magnitudes; values exceeding ±20 before softmax are red flags.\\n\\n**Dimension mismatches** are the most common bug during multi-head attention. Track dimensions religiously through each operation:\\n- Input: `(batch, seq_len, d_model)`\\n- After Q/K/V projection: `(batch, seq_len, d_model)`\\n- After splitting into heads: `(batch, num_heads, seq_len, d_k)`\\n- Post-matmul attention: `(batch, num_heads, seq_len, d_k)`\\n\\nThe reshape between representations is where errors hide. Draw boxes for each tensor shape and verify your view/reshape operations preserve total element counts.\\n\\n**Mask errors** are insidious. A classic mistake: inverting the mask logic, causing the model to attend only to padding tokens while ignoring content. Masks should zero out unwanted positions; verify by printing a few attention weight matrices and confirming masked positions are exactly 0.0. Also check broadcasting—your `(seq_len, seq_len)` causal mask must align with the `(batch, num_heads, seq_len, seq_len)` attention tensor.\\n\\n**Gradient vanishing** correlates with attention entropy. Calculate entropy of attention distributions: H = -Σ(p_i * log(p_i)). Healthy attention shows entropy between 1.0 and log(seq_len). Near-zero entropy (peaked distributions) can block gradient flow to earlier positions. If softmax saturates due to unscaled logits, gradients approach zero.\\n\\n**Numerical stability** breaks when large logits cause exp() overflow. Implement the log-sum-exp trick: subtract the max logit before exponentiating. This shifts all values into a safe range without changing the final probabilities.\\n\\n**Verification tests** should be your first debugging step. Write assertions:\\n- `assert attention_weights.sum(dim=-1).allclose(1.0)` (each row sums to 1)\\n- `assert (attention_weights * mask).sum() == attention_weights.sum()` (masks truly zero out)\\n\\nThese checks catch 80% of bugs before they propagate downstream.'),\n",
       "  (8,\n",
       "   '## Self-Attention Variants and Extensions\\n\\nThe core self-attention mechanism we\\'ve explored forms the foundation for numerous specialized variants, each designed to address specific computational or architectural challenges.\\n\\n**Cross-attention** modifies the basic pattern by sourcing queries from one sequence while keys and values come from another. In encoder-decoder architectures, the decoder uses cross-attention to attend over encoder outputs—queries represent \"what I\\'m currently generating\" while keys/values represent \"what context is available.\" This same mechanism powers retrieval-augmented generation (RAG), where queries come from your generation context and keys/values from retrieved documents, allowing models to ground responses in external knowledge.\\n\\n**Sparse attention** tackles the quadratic complexity problem by restricting which positions can attend to each other. Instead of computing all n² attention scores, sparse patterns might limit attention to a fixed window (local attention) or use structured patterns like strided blocks. While you lose full connectivity, many tasks don\\'t require every token to directly attend to every other token—language often has strong local dependencies.\\n\\n**Flash Attention** represents a different optimization angle: rather than reducing the number of computations, it reorganizes how attention is computed to minimize memory bandwidth. By tiling the computation and fusing operations, Flash Attention computes exact attention without ever materializing the full score matrix in high-bandwidth memory. This yields significant speedups on modern GPUs where memory movement often dominates compute time.\\n\\n**Grouped-query attention** (GQA) reduces memory overhead during inference by sharing key and value projection weights across multiple query heads. In a standard multi-head setup with 32 heads, you store 32 different K and V caches. GQA might use only 4 or 8 KV heads, with query heads grouped to share them. This dramatically shrinks the KV cache size—critical for serving long-context requests—while maintaining most of the representational power.'),\n",
       "  (9,\n",
       "   \"## When to Use Self-Attention vs Alternatives\\n\\nSelf-attention shines in scenarios where you need to model **variable-length dependencies** across entire sequences. If your task requires understanding relationships between tokens that might be 10 or 1,000 positions apart—like coreference resolution in long documents or capturing melodic themes in music—self-attention's global context window makes it the natural choice. The mechanism's ability to compute all pairwise interactions in parallel also dramatically accelerates training compared to sequential architectures, cutting wall-clock time from days to hours for many NLP tasks.\\n\\nHowever, **RNNs and LSTMs remain relevant** in specific contexts. Streaming applications that process data token-by-token with strict causality constraints—like real-time speech recognition or live caption generation—benefit from RNNs' inherently sequential structure with constant memory footprint. For extremely long sequences (100K+ tokens) where quadratic attention costs become prohibitive and you have limited GPU budget, architectures like LSTMs with linear complexity or sparse attention variants may be more practical.\\n\\n**Hybrid architectures** often provide the best of both worlds. Computer vision models increasingly use convolutional layers for low-level feature extraction (edges, textures) followed by transformer blocks for high-level reasoning about spatial relationships—capturing both local inductive biases and global dependencies. Similarly, audio models like Conformers interleave convolutions with self-attention to efficiently model both short-term acoustic patterns and long-range temporal structure.\\n\\n**Production constraints** frequently override theoretical optimality. A model serving 1M requests per second needs sub-10ms latency; the quadratic memory scaling of self-attention might force you toward distilled models, pruned attention heads, or even switching to linear-complexity alternatives despite accuracy trade-offs. Always profile actual inference costs—memory bandwidth often matters more than FLOP count.\\n\\n**Domain-specific considerations** guide architecture choices. In vision, ViTs excel when pretrained on massive datasets but CNNs remain competitive on smaller datasets due to stronger spatial inductive biases. For structured data like molecules or knowledge graphs, incorporating relational priors (bond types, edge features) into attention may outperform vanilla self-attention by reducing the burden of learning domain structure from scratch.\")],\n",
       " 'final': '# Understanding Self-Attention: The Mechanism Behind Modern Transformers\\n\\n## The Problem Self-Attention Solves\\n\\nBefore transformers revolutionized natural language processing, two architectures dominated sequence modeling: recurrent neural networks (RNNs) and convolutional neural networks (CNNs). Both faced fundamental limitations that self-attention was designed to overcome.\\n\\n**The Sequential Bottleneck**\\n\\nRNNs process sequences token-by-token, maintaining a hidden state that flows from one timestep to the next. This sequential dependency creates a critical bottleneck: you cannot compute the representation for token 100 until you\\'ve processed tokens 1 through 99. During training, this means no parallelization across the sequence dimension—you\\'re stuck processing one step at a time, even with modern GPU hardware.\\n\\nWorse, as sequences grow longer, information must pass through many intermediate states. The hidden state acts like a telephone game where the message degrades over distance. By the time an RNN processes the 100th token, subtle information from token 5 has been compressed and recompressed through 95 sequential transformations, making long-range dependencies difficult to learn.\\n\\n**Fixed Windows and Receptive Fields**\\n\\nCNNs avoid sequential processing by applying filters in parallel, but they introduce a different constraint: fixed receptive fields. A convolutional layer with kernel size 3 only sees three adjacent tokens. To capture dependencies across 20 tokens, you need to stack multiple layers, and the receptive field grows slowly. The architecture imposes a rigid geometric structure on which tokens can interact.\\n\\n**The Core Insight**\\n\\nSelf-attention solves both problems with a deceptively simple idea: let every token directly compare itself to every other token in the sequence, and learn which positions matter. Instead of processing left-to-right or through fixed windows, the model computes attention scores dynamically based on content similarity.\\n\\nConsider the sentence: \"The animal didn\\'t cross the street because it was too tired.\" To understand what \"it\" refers to, you need to connect tokens 8 (\"it\") and 2 (\"animal\"), skipping over intervening words. Self-attention allows the model to learn this connection directly, regardless of distance, in a single operation that parallelizes across all tokens simultaneously.\\n\\n## Self-Attention Mechanics: Queries, Keys, and Values\\n\\nThe self-attention mechanism borrows its core metaphor from information retrieval systems. Imagine searching a library: you have a **query** (what you\\'re looking for), **keys** (index cards describing each book), and **values** (the actual book contents). Self-attention works the same way—each token generates a query to search over all other tokens\\' keys, then retrieves a weighted combination of their values.\\n\\n### The Three Linear Projections\\n\\nSelf-attention starts by transforming each token\\'s embedding through three separate learned weight matrices: **Wq** (query), **Wk** (key), and **Wv** (value). If your input embeddings have dimension d_model, these matrices typically project to a smaller dimension d_k (often d_model/num_heads).\\n\\nFor an input token embedding **x**, we compute:\\n- Query: **q** = **x** · Wq\\n- Key: **k** = **x** · Wk  \\n- Value: **v** = **x** · Wv\\n\\nEvery token in the sequence undergoes this transformation independently, producing matrices Q, K, and V where each row corresponds to one token\\'s projection.\\n\\n### Computing Attention Scores\\n\\nThe heart of self-attention is measuring how much each token should attend to every other token. We compute similarity by taking the dot product between each query and all keys: **q** · **k**^T. High dot products indicate strong relevance; the query \"found\" something useful in that key.\\n\\nFor a sequence of n tokens, this produces an n×n matrix of raw attention scores. Each row represents one query\\'s scores across all keys—essentially asking \"how relevant is every position to this position?\"\\n\\nThese raw scores get scaled by 1/√d_k to prevent extremely large values that would make gradients vanish during training. Then we apply **softmax** row-wise, converting each row of scores into a probability distribution that sums to 1. This normalization ensures attention weights are interpretable and numerically stable.\\n\\n### Weighted Value Aggregation\\n\\nThe final step multiplies the attention weights by the value matrix V. Each output position receives a weighted sum of all value vectors, where the weights come from the normalized attention scores. Positions with high attention weights contribute more to the output.\\n\\n### Concrete Example\\n\\nConsider three tokens with 4-dimensional embeddings. After projections to d_k=2:\\n\\n```\\nQ = [[1.0, 0.5],     K = [[1.0, 0.2],     V = [[2.0, 1.0],\\n     [0.3, 0.8],          [0.5, 0.9],          [1.5, 0.5],\\n     [0.6, 0.4]]          [0.4, 0.3]]          [1.0, 2.0]]\\n```\\n\\nCompute scores: Q·K^T / √2 gives a 3×3 matrix. For token 0: [1.0×1.0 + 0.5×0.2, 1.0×0.5 + 0.5×0.9, ...] = [0.78, 0.67, 0.53]. After softmax: [0.37, 0.33, 0.30]. The output for token 0 becomes: 0.37×[2.0,1.0] + 0.33×[1.5,0.5] + 0.30×[1.0,2.0] ≈ [1.64, 1.14], a context-aware representation blending information from all positions.\\n\\n## Implementing Scaled Dot-Product Attention\\n\\nLet\\'s build the core attention mechanism from scratch. We\\'ll start with a clean NumPy implementation, then show the PyTorch equivalent that handles batching efficiently.\\n\\nHere\\'s the fundamental scaled dot-product attention function:\\n\\n```python\\nimport numpy as np\\n\\ndef attention(Q, K, V):\\n    \"\"\"\\n    Q: Query matrix of shape (seq_len, d_k)\\n    K: Key matrix of shape (seq_len, d_k)\\n    V: Value matrix of shape (seq_len, d_v)\\n    Returns: Output of shape (seq_len, d_v), attention weights\\n    \"\"\"\\n    d_k = Q.shape[-1]\\n    \\n    # Step 1: Compute attention scores - shape (seq_len, seq_len)\\n    scores = Q @ K.T  # Matrix multiplication\\n    \\n    # Step 2: Scale by sqrt(d_k) - shape unchanged\\n    scaled_scores = scores / np.sqrt(d_k)\\n    \\n    # Step 3: Apply softmax row-wise - shape (seq_len, seq_len)\\n    attention_weights = np.exp(scaled_scores) / np.exp(scaled_scores).sum(axis=-1, keepdims=True)\\n    \\n    # Step 4: Weight the values - shape (seq_len, d_v)\\n    output = attention_weights @ V\\n    \\n    return output, attention_weights\\n```\\n\\n**Why the sqrt(d_k) scaling factor?** Without scaling, dot products grow large as dimensionality increases. Consider two random unit vectors in high dimensions—their dot product variance scales with `d_k`. Large scores push softmax into regions with vanishingly small gradients (imagine softmax([100, 1, 1])—it\\'s essentially a hard selection with near-zero gradient for non-max positions). Dividing by `sqrt(d_k)` keeps the variance of scores roughly constant regardless of dimension, maintaining healthy gradients during training.\\n\\nFor production use with batched inputs, here\\'s the PyTorch implementation:\\n\\n```python\\nimport torch\\nimport torch.nn.functional as F\\n\\ndef scaled_dot_product_attention(Q, K, V, mask=None):\\n    \"\"\"\\n    Q, K, V: shape (batch_size, seq_len, d_k) or (batch_size, num_heads, seq_len, d_k)\\n    mask: optional boolean mask, shape (seq_len, seq_len) or (batch_size, seq_len, seq_len)\\n    \"\"\"\\n    d_k = Q.size(-1)\\n    \\n    # scores: (batch, seq_len, seq_len) or (batch, num_heads, seq_len, seq_len)\\n    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\\n    \\n    # Apply mask before softmax (set masked positions to -inf)\\n    if mask is not None:\\n        scores = scores.masked_fill(mask == 0, float(\\'-inf\\'))\\n    \\n    attention_weights = F.softmax(scores, dim=-1)\\n    output = torch.matmul(attention_weights, V)\\n    \\n    return output, attention_weights\\n```\\n\\n**Masking for different use cases:** Causal masks prevent attention to future positions (essential for autoregressive generation like GPT):\\n\\n```python\\ndef create_causal_mask(seq_len):\\n    # Lower triangular matrix: position i can only attend to j <= i\\n    return torch.tril(torch.ones(seq_len, seq_len)).bool()\\n\\n# Padding masks ignore <PAD> tokens\\ndef create_padding_mask(seq_lengths, max_len):\\n    batch_size = len(seq_lengths)\\n    mask = torch.arange(max_len).expand(batch_size, max_len) < seq_lengths.unsqueeze(1)\\n    return mask.unsqueeze(1)  # (batch, 1, seq_len) for broadcasting\\n```\\n\\n**Verification checks:** The attention weight matrix should always satisfy two properties: (1) each row sums to 1.0 (it\\'s a probability distribution), and (2) all values are non-negative. You can verify with:\\n\\n```python\\n# After computing attention_weights\\nassert torch.allclose(attention_weights.sum(dim=-1), torch.ones(attention_weights.shape[:-1]))\\nassert (attention_weights >= 0).all()\\n```\\n\\nThe output shape should match the value dimension: if V has shape `(batch, seq_len, d_v)`, output will too. Each output position is a weighted sum of all value vectors, where weights come from comparing that position\\'s query against all keys.\\n\\n## Multi-Head Attention: Parallel Representation Subspaces\\n\\nSingle attention heads face a fundamental limitation: they must make trade-offs about what patterns to capture. A single set of query, key, and value projections might learn to identify subject-verb relationships in a sentence, but simultaneously capturing semantic similarity, syntactic structure, and positional patterns proves difficult. Multi-head attention solves this by running several attention mechanisms in parallel, each learning to specialize in different types of relationships.\\n\\n**Representation Splitting Across Heads**\\n\\nInstead of computing attention once with full dimensionality, multi-head attention divides the model dimension `d_model` into `h` independent heads. Each head operates on a smaller dimension `d_k = d_model / h`. For example, with `d_model = 512` and `h = 8` heads, each head works with `d_k = 64` dimensions.\\n\\nThe key insight is that each head gets its own learned query, key, and value projection matrices. Head 1 might learn projections that emphasize word order and position, while Head 2\\'s projections capture semantic relationships between entities. Head 3 might specialize in long-range dependencies. This division creates independent \"representation subspaces\" where different aspects of the input can be captured simultaneously.\\n\\n**Efficient Parallel Implementation**\\n\\nRather than iterating through heads sequentially, implementations reshape tensors to compute all heads at once. After projecting queries, keys, and values, the tensors are reshaped from `[batch, seq_len, d_model]` to `[batch, num_heads, seq_len, d_k]`. This reorganization allows the matrix multiplication operations in scaled dot-product attention to process all heads in parallel using batched operations, maintaining computational efficiency despite running multiple attention mechanisms.\\n\\n**Combining Head Outputs**\\n\\nAfter each head computes its attention-weighted values, the outputs are concatenated back together. If Head 1 produces a `[batch, seq_len, d_k]` tensor and we have `h` heads, concatenation yields `[batch, seq_len, h * d_k] = [batch, seq_len, d_model]`. A final learned linear projection `W_O` then mixes information across all heads, allowing them to jointly determine the output representation.\\n\\n**Empirical Specialization Patterns**\\n\\nAnalysis of trained transformers reveals fascinating head specialization. Some heads consistently attend to the previous token (capturing sequential dependencies), others attend to sentence-ending punctuation (syntax structure), and some form broad attention patterns capturing semantic fields. This emergent specialization, learned purely from data without explicit supervision, demonstrates why multiple heads substantially improve model expressiveness compared to single-head architectures.\\n\\n## Computational Complexity and Memory Footprint\\n\\nSelf-attention\\'s elegant design comes with a computational price tag that every practitioner must understand. The mechanism computes pairwise relationships between all tokens in a sequence, leading to **quadratic complexity** in both time and space.\\n\\n### Time Complexity Analysis\\n\\nFor a sequence of length `n` with embedding dimension `d`, computing the full attention matrix requires O(n²·d) operations. Here\\'s the breakdown:\\n\\n- **Query-Key dot products**: n² dot products, each requiring d multiplications → O(n²·d)\\n- **Softmax normalization**: Applied across n² scores → O(n²)\\n- **Value aggregation**: n² weighted combinations of d-dimensional vectors → O(n²·d)\\n\\nThe dominant term is O(n²·d), meaning doubling your sequence length quadruples the compute time.\\n\\n### Memory Requirements\\n\\nThe n×n attention matrix must be materialized and stored during the forward pass for gradient computation during backpropagation. For a 2048-token sequence, that\\'s 4 million floating-point values per attention head. With 8 heads and 32-bit floats, you\\'re storing ~128MB just for attention weights in a single layer. Deep models with 24+ layers compound this quickly.\\n\\n### Comparison with Alternatives\\n\\nThis contrasts sharply with sequential architectures:\\n\\n- **RNNs**: O(n·d²) complexity—linear in sequence length but quadratic in hidden dimension\\n- **CNNs**: O(n·k·d²) where k is the kernel width (typically 3-7)—linear in sequence length with localized context\\n\\nFor short sequences (n < 512) with large hidden dimensions (d ≈ 1024), RNNs can actually be more expensive per token. But as sequences grow, self-attention\\'s quadratic term dominates.\\n\\n### Practical Break-Even Points\\n\\nOn standard GPUs (V100/A100), self-attention becomes prohibitively expensive beyond 2048 tokens for training, and inference slows noticeably past 4096 tokens. These limits have driven innovation in **efficiency techniques**: sparse attention patterns that compute only a subset of token pairs, linear attention approximations that reduce complexity to O(n·d²), and memory-efficient attention implementations like Flash Attention that reduce memory overhead through kernel fusion and recomputation strategies.\\n\\n## Positional Information and Permutation Invariance\\n\\nA fundamental property of self-attention that often surprises newcomers: **the mechanism is completely position-agnostic**. If you shuffle the input tokens randomly, self-attention produces the exact same outputs in the shuffled order. This permutation invariance stems from how attention treats inputs as an unordered set—each token attends to all others based purely on content similarity, with no inherent notion of which token came \"before\" or \"after\" another.\\n\\nThis creates an immediate problem for language understanding. The sentences \"dog bites man\" and \"man bites dog\" convey opposite meanings, yet without positional information, self-attention computes identical representations for both. Each word looks at the same neighboring words and generates the same attention weights, just in different orders. The mechanism has no way to distinguish sequential relationships that fundamentally change meaning.\\n\\nTo restore position awareness, transformers inject **positional encodings** into the input embeddings before attention begins. The most common approaches fall into two categories:\\n\\n**Absolute positional encodings** assign each position an embedding vector that gets added to the token embedding. The original Transformer paper introduced sinusoidal functions—deterministic patterns at different frequencies that let the model distinguish positions mathematically. Alternatively, many modern models use learned position embeddings, treating them as trainable parameters optimized during training. These approaches are simple to implement: just add the position vector to each token embedding before the first attention layer.\\n\\n**Relative position representations** take a different approach by modifying the attention mechanism itself to account for the distance between tokens. Instead of asking \"what\\'s at position 5?\", the model asks \"what\\'s 3 positions away from me?\" This can be implemented by adjusting attention scores based on relative offsets or by incorporating relative position embeddings into the key/query computations.\\n\\nThe trade-off centers on generalization. Absolute encodings are straightforward but struggle when inference sequences exceed training lengths—position 10,000 looks alien if you only trained on sequences up to 2,048 tokens. Relative encodings can naturally extrapolate since they reason about distances rather than absolute indices, though they add computational complexity to the attention calculation itself. Modern architectures like RoPE (Rotary Position Embedding) attempt to get the best of both worlds by encoding relative information through rotation matrices applied to query and key vectors.\\n\\n## Common Implementation Pitfalls and Debugging\\n\\nWhen implementing self-attention from scratch, several failure modes appear repeatedly. Recognizing these patterns early saves hours of head-scratching debugging.\\n\\n**Attention weight degeneracy** manifests as pathological distributions—either all weights collapsing to uniform values (1/N across all positions) or spiking to a single position. The uniform case often signals dead neurons or vanishing gradients upstream. Single-peaked attention may indicate runaway scaling: if your attention logits grow too large before softmax, numerical precision causes one weight to dominate. Check your scaling factor—√d_k is standard, but verify it matches your actual key dimension. Inspect raw logit magnitudes; values exceeding ±20 before softmax are red flags.\\n\\n**Dimension mismatches** are the most common bug during multi-head attention. Track dimensions religiously through each operation:\\n- Input: `(batch, seq_len, d_model)`\\n- After Q/K/V projection: `(batch, seq_len, d_model)`\\n- After splitting into heads: `(batch, num_heads, seq_len, d_k)`\\n- Post-matmul attention: `(batch, num_heads, seq_len, d_k)`\\n\\nThe reshape between representations is where errors hide. Draw boxes for each tensor shape and verify your view/reshape operations preserve total element counts.\\n\\n**Mask errors** are insidious. A classic mistake: inverting the mask logic, causing the model to attend only to padding tokens while ignoring content. Masks should zero out unwanted positions; verify by printing a few attention weight matrices and confirming masked positions are exactly 0.0. Also check broadcasting—your `(seq_len, seq_len)` causal mask must align with the `(batch, num_heads, seq_len, seq_len)` attention tensor.\\n\\n**Gradient vanishing** correlates with attention entropy. Calculate entropy of attention distributions: H = -Σ(p_i * log(p_i)). Healthy attention shows entropy between 1.0 and log(seq_len). Near-zero entropy (peaked distributions) can block gradient flow to earlier positions. If softmax saturates due to unscaled logits, gradients approach zero.\\n\\n**Numerical stability** breaks when large logits cause exp() overflow. Implement the log-sum-exp trick: subtract the max logit before exponentiating. This shifts all values into a safe range without changing the final probabilities.\\n\\n**Verification tests** should be your first debugging step. Write assertions:\\n- `assert attention_weights.sum(dim=-1).allclose(1.0)` (each row sums to 1)\\n- `assert (attention_weights * mask).sum() == attention_weights.sum()` (masks truly zero out)\\n\\nThese checks catch 80% of bugs before they propagate downstream.\\n\\n## Self-Attention Variants and Extensions\\n\\nThe core self-attention mechanism we\\'ve explored forms the foundation for numerous specialized variants, each designed to address specific computational or architectural challenges.\\n\\n**Cross-attention** modifies the basic pattern by sourcing queries from one sequence while keys and values come from another. In encoder-decoder architectures, the decoder uses cross-attention to attend over encoder outputs—queries represent \"what I\\'m currently generating\" while keys/values represent \"what context is available.\" This same mechanism powers retrieval-augmented generation (RAG), where queries come from your generation context and keys/values from retrieved documents, allowing models to ground responses in external knowledge.\\n\\n**Sparse attention** tackles the quadratic complexity problem by restricting which positions can attend to each other. Instead of computing all n² attention scores, sparse patterns might limit attention to a fixed window (local attention) or use structured patterns like strided blocks. While you lose full connectivity, many tasks don\\'t require every token to directly attend to every other token—language often has strong local dependencies.\\n\\n**Flash Attention** represents a different optimization angle: rather than reducing the number of computations, it reorganizes how attention is computed to minimize memory bandwidth. By tiling the computation and fusing operations, Flash Attention computes exact attention without ever materializing the full score matrix in high-bandwidth memory. This yields significant speedups on modern GPUs where memory movement often dominates compute time.\\n\\n**Grouped-query attention** (GQA) reduces memory overhead during inference by sharing key and value projection weights across multiple query heads. In a standard multi-head setup with 32 heads, you store 32 different K and V caches. GQA might use only 4 or 8 KV heads, with query heads grouped to share them. This dramatically shrinks the KV cache size—critical for serving long-context requests—while maintaining most of the representational power.\\n\\n## When to Use Self-Attention vs Alternatives\\n\\nSelf-attention shines in scenarios where you need to model **variable-length dependencies** across entire sequences. If your task requires understanding relationships between tokens that might be 10 or 1,000 positions apart—like coreference resolution in long documents or capturing melodic themes in music—self-attention\\'s global context window makes it the natural choice. The mechanism\\'s ability to compute all pairwise interactions in parallel also dramatically accelerates training compared to sequential architectures, cutting wall-clock time from days to hours for many NLP tasks.\\n\\nHowever, **RNNs and LSTMs remain relevant** in specific contexts. Streaming applications that process data token-by-token with strict causality constraints—like real-time speech recognition or live caption generation—benefit from RNNs\\' inherently sequential structure with constant memory footprint. For extremely long sequences (100K+ tokens) where quadratic attention costs become prohibitive and you have limited GPU budget, architectures like LSTMs with linear complexity or sparse attention variants may be more practical.\\n\\n**Hybrid architectures** often provide the best of both worlds. Computer vision models increasingly use convolutional layers for low-level feature extraction (edges, textures) followed by transformer blocks for high-level reasoning about spatial relationships—capturing both local inductive biases and global dependencies. Similarly, audio models like Conformers interleave convolutions with self-attention to efficiently model both short-term acoustic patterns and long-range temporal structure.\\n\\n**Production constraints** frequently override theoretical optimality. A model serving 1M requests per second needs sub-10ms latency; the quadratic memory scaling of self-attention might force you toward distilled models, pruned attention heads, or even switching to linear-complexity alternatives despite accuracy trade-offs. Always profile actual inference costs—memory bandwidth often matters more than FLOP count.\\n\\n**Domain-specific considerations** guide architecture choices. In vision, ViTs excel when pretrained on massive datasets but CNNs remain competitive on smaller datasets due to stronger spatial inductive biases. For structured data like molecules or knowledge graphs, incorporating relational priors (bond types, edge features) into attention may outperform vanilla self-attention by reducing the burden of learning domain structure from scratch.\\n'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\"Write a blog on Self Attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18b4d07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
